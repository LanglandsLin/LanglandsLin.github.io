<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0050)http://www.icst.pku.edu.cn/course/icb/MRS_MCI.html -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="language" content="english">
<title>acmmm2020_lll</title>
<meta name="description" content="acmmm_lll">
<meta name="author" content="Lilang Lin">
<link rel="icon" type="image/x-icon" href="http://www.icst.pku.edu.cn/favicon.ico">
<link rel="stylesheet" type="text/css" href="files/project.css">

<script> 
	function coming_soon()
	{
		alert("We are cleaning up our code to make it more simple and readable");
	}
	</script>
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
	<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$']]},
        messageStyle: "none"
    });</script>

</head>


	
	

<body>
<div id="main">
  
	<div class="content"><br>
		<div class="title">
			<p class="banner" align="center"></p>
			<h1>MS<sup>2</sup>L: Multi-Task Self-Supervised Learning for Skeleton Based Action Recognition</h1>
		</div>

		<div class="authors" >
            <div class="author">
               <a href="mailto:linlilang@pku.edu.cn" style="text-decoration: none">Lilang Lin</a>
            </div>
            <div class="author">
               <a href="mailto:ssj940920@jd.com" style="text-decoration: none">Sijie Song</a>
			</div>
			<div class="author">
				<a href="mailto:yangwenhan@pku.edu.cn" style="text-decoration: none">Wenhan Yang</a>
			 </div>
            <div class="author">
               <a href="mailto:liujiaying@pku.edu.cn" style="text-decoration: none">Jiaying Liu</a>
            </div>
            <p style="margin-bottom:-20px">Accepted to <em>ACM-MM 2020</em></p>
		 </div>
		<br><hr>
		<div class="teaser sec">
			<img src="./files/teaser.png" alt="Teaser" width="100%">
		  	<p style="text-align: left font-size:15%:">Fig.1  Our proposed approach (MS<sup>2</sup>L) learns more general and discriminative features benefiting from multiple self-supervised tasks.</p>
	  	</div>

		<div class="abstract_sec">
			<h2>Abstract</h2>
			<div class="desp">
				<p style="text-align:justify">
					In this paper, we address self-supervised representation learning from human skeletons for action recognition. Previous methods, which usually learn feature presentations from a single reconstruction task, may come across the overfitting problem, and the features are not generalizable for action recognition. Instead, we propose to integrate multiple tasks to learn more general representations in a self-supervised manner. To realize this goal, we integrate motion prediction, jigsaw puzzle recognition, and contrastive learning to learn skeleton features from different aspects. Skeleton dynamics can be modeled through motion prediction by predicting the future sequence. And temporal patterns, which are critical for action recognition, are learned through solving jigsaw puzzles. We further regularize the feature space by contrastive learning. Besides, we explore different training strategies to utilize the knowledge from self-supervised tasks for action recognition. We evaluate our multi-task self-supervised learning approach with action classifiers trained under different configurations, including unsupervised, semi-supervised and fully-supervised settings. Our experiments on the NW-UCLA, NTU RGB+D, and PKUMMD datasets show remarkable performance for action recognition, demonstrating the superiority of our method in learning more discriminative and general features.
				</p>
			</div>
		</div>
	
		<div class="Framework sec">
			<h2>Framework</h2>
			<img src="./files/pipeline.png" alt="Framework" width="100%">
			<p style="text-align: left font-size:15%:">Fig.2 The structure of our network. (a) Encoder. (b) Multi-task heads. (c) Classifier for action recognition. We use yellow, blue and grey arrows to indicate the pipeline for motion prediction, jigsaw puzzle recognition and contrastive learning, respectively. Action recognition is achieved with the red pipeline.</p>
	  	</div>

		<div class="experiments_sec">
			<h2>Results</h2>
			<div class="picture_wrapper" width="100%">
				<img src="./files/table12.png" alt="Results" width="100%">
				<img src="./files/table3.png" alt="Results" width="60%">
				<img src="./files/table45.png" alt="Results" width="60%">
				<img src="./files/table6.png" alt="Results" width="60%">
			</div>
		</div>
		  
		
	  	<div class="download_sec">
			<h2>Resources</h2>
			<div>
				<li><strong>Paper</strong>:  <a href="https://arxiv.org/abs/2010.05599">arXiv</a></li>
				<li><strong>Supplementary material</strong>:  coming soon</li>
				<li><strong>Code</strong>:  <a href="https://github.com/LanglandsLin/MS2L">github</a></li>

			</div>
		</div>

		<div class="citation_sec">
			<h2>Citation</h2>
			<p class="bibtex">@inproceedings{MS2L, 
&nbsp;&nbsp; author={Lilang Lin and Sijie Song and Wenhan Yang and Jiaying Liu}, 
&nbsp;&nbsp; booktitle={ACM Multimedia}, 
&nbsp;&nbsp; title={MS^2L: Multi-Task Self-Supervised Learning for Skeleton Based Action Recognition}, 
&nbsp;&nbsp; year={2020}, 
&nbsp;&nbsp; }
			</p>
	    </div>
		
       <div>
          <h2>Reference</h2>
            <p>[1]. T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual representations. ICML 2020.</p>
            <p>[2]. C. Doersch, A. Gupta, and A. A Efros. Unsupervised visual representation learning by context prediction. ICCV 2015.</p>
            <p>[3]. Y. Du, Y. Fu, and L. Wang. Representation learning of temporal dynamics for skeleton-based action recognition. TIP 2016.</p>
            <p>[4]. Y. Du, W. Wang, and L. Wang. Hierarchical Recurrent Neural Network for Skeleton Based Action Recognition. CVPR 2015.</p>
        </div>
	
  </div>
</div>


</body></html>
